{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn the Vocabulary**\n",
    "\n",
    "We use the Multi30k dataset for english - german translation. This notebook uses the byte-pair encoding to fit a joint vocabulary for both the English and German language based on the most frequent byte pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniforge3/envs/nlp-experiments/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import multi30k, Multi30k\n",
    "\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "multi30k.URL[\"test\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n",
    "\n",
    "data = Multi30k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'Two young, White males are outside near many bushes.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'Two young, White males are outside near many bushes.',\n",
       " 'Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.',\n",
       " 'Several men in hard hats are operating a giant pulley system.',\n",
       " 'Ein kleines Mädchen klettert in ein Spielhaus aus Holz.',\n",
       " 'A little girl climbing into a wooden playhouse.',\n",
       " 'Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.',\n",
       " 'A man in a blue shirt is standing on a ladder cleaning a window.',\n",
       " 'Zwei Männer stehen am Herd und bereiten Essen zu.',\n",
       " 'Two men are at the stove preparing food.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_joint_text_corpus(data):\n",
    "    joint_text = []\n",
    "    for pair in data:\n",
    "        joint_text.append(pair[0])\n",
    "        joint_text.append(pair[1])\n",
    "    return joint_text\n",
    "\n",
    "text_joint_en_de = get_joint_text_corpus(train_data)\n",
    "text_joint_en_de[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we turn a text corpus into a fixed length vector?**\n",
    "\n",
    "With the Byte-Pair Encoding. Let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus_from_data(data, language, n_sentences):\n",
    "    corpus = []\n",
    "    for i in range(n_sentences):\n",
    "        corpus.append(data[i][0 if language == \"german\" else 1])\n",
    "    return corpus\n",
    "\n",
    "N_SENTENCES = 1000\n",
    "english_corpus = extract_corpus_from_data(train_data, \"english\", N_SENTENCES)\n",
    "german_corpus = extract_corpus_from_data(train_data, \"german\", N_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/894 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 14/894 [00:27<32:02,  2.18s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m Path(data_path)\u001b[39m.\u001b[39mexists():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     encoder_universal \u001b[39m=\u001b[39m BytePairEncoder(\u001b[39m\"\u001b[39m\u001b[39muniversal\u001b[39m\u001b[39m\"\u001b[39m, max_vocab_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, use_start_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_end_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_padding_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_token_len\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     time \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39;49mtimeit(\u001b[39mlambda\u001b[39;49;00m: encoder_universal\u001b[39m.\u001b[39;49mlearn_vocabulary_from_corpus(text_joint_en_de, n_processes\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m), number\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime to learn vocabulary for universal BPE: \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     encoder_universal\u001b[39m.\u001b[39msave_vocabulary(data_path)\n",
      "File \u001b[0;32m/opt/miniforge3/envs/nlp-experiments/lib/python3.11/timeit.py:234\u001b[0m, in \u001b[0;36mtimeit\u001b[0;34m(stmt, setup, timer, number, globals)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtimeit\u001b[39m(stmt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpass\u001b[39m\u001b[39m\"\u001b[39m, setup\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpass\u001b[39m\u001b[39m\"\u001b[39m, timer\u001b[39m=\u001b[39mdefault_timer,\n\u001b[1;32m    232\u001b[0m            number\u001b[39m=\u001b[39mdefault_number, \u001b[39mglobals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    233\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convenience function to create Timer object and call timeit method.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m Timer(stmt, setup, timer, \u001b[39mglobals\u001b[39;49m)\u001b[39m.\u001b[39;49mtimeit(number)\n",
      "File \u001b[0;32m/opt/miniforge3/envs/nlp-experiments/lib/python3.11/timeit.py:178\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    176\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer, _stmt)\u001b[0m\n",
      "\u001b[1;32m/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m Path(data_path)\u001b[39m.\u001b[39mexists():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     encoder_universal \u001b[39m=\u001b[39m BytePairEncoder(\u001b[39m\"\u001b[39m\u001b[39muniversal\u001b[39m\u001b[39m\"\u001b[39m, max_vocab_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, use_start_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_end_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_padding_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_token_len\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     time \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mtimeit(\u001b[39mlambda\u001b[39;00m: encoder_universal\u001b[39m.\u001b[39;49mlearn_vocabulary_from_corpus(text_joint_en_de, n_processes\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m), number\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime to learn vocabulary for universal BPE: \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/core/Development/NLP/nlp_playground/learn_vocabulary.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     encoder_universal\u001b[39m.\u001b[39msave_vocabulary(data_path)\n",
      "File \u001b[0;32m~/Development/NLP/nlp_playground/byte_pair_encoder.py:212\u001b[0m, in \u001b[0;36mBytePairEncoder.learn_vocabulary_from_corpus\u001b[0;34m(self, corpus, n_processes)\u001b[0m\n\u001b[1;32m    210\u001b[0m chunk_indices \u001b[39m=\u001b[39m [(i \u001b[39m*\u001b[39m chunk_size, \u001b[39mmin\u001b[39m((i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m chunk_size, \u001b[39mlen\u001b[39m(corpus))) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_processes)]\n\u001b[1;32m    211\u001b[0m \u001b[39mwith\u001b[39;00m Pool(n_processes) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m--> 212\u001b[0m     results \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mstarmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_pairs_parallel, [(corpus, start_idx, end_idx) \u001b[39mfor\u001b[39;49;00m start_idx, end_idx \u001b[39min\u001b[39;49;00m chunk_indices])\n\u001b[1;32m    213\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results:\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/miniforge3/envs/nlp-experiments/lib/python3.11/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/opt/miniforge3/envs/nlp-experiments/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniforge3/envs/nlp-experiments/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m/opt/miniforge3/envs/nlp-experiments/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/miniforge3/envs/nlp-experiments/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from byte_pair_encoder import BytePairEncoder\n",
    "import timeit\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = \"data/universal_bpe_encoder.pkl\"\n",
    "if not Path(data_path).exists():\n",
    "    encoder_universal = BytePairEncoder(\"universal\", max_vocab_size=1000, use_start_token=True, use_end_token=True, use_padding_token=True, max_token_len=50)\n",
    "    time = timeit.timeit(lambda: encoder_universal.learn_vocabulary_from_corpus(text_joint_en_de, n_processes=8), number=1)\n",
    "    print(f\"Time to learn vocabulary for universal BPE: {time:.2f} seconds\")\n",
    "    encoder_universal.save_vocabulary(data_path)\n",
    "else:\n",
    "    encoder_universal = BytePairEncoder.load_vocabulary(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
